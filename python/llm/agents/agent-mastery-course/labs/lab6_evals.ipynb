{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR9M6va6Apos"
      },
      "source": [
        "<center>\n",
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmrxohFyArrn"
      },
      "source": [
        "# **Arize Agent Mastry Course: Evaluating Your Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBYtGoYBMPL"
      },
      "source": [
        "So far, we have built our agent, added tooling, and implemented a RAG system that allows it to access information. Now, we are ready to run the agent and evaluate its outputs. Evaluations can take different forms such as LLM, code, or human, and can be applied at various scopes including trace, span, and session.\n",
        "\n",
        "In this lab, we will demonstrate how to run evaluations in code and log the results to the Arize UI. We will also show you how to set up and run evaluations directly within the Arize UI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBIekm-CAlU"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PieRuLGf7ugU"
      },
      "outputs": [],
      "source": [
        "!pip install -qqqqqqqq arize-otel arize agno openai openinference-instrumentation-agno openinference-instrumentation-openai httpx chromadb sentence-transformers arize-phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfgANTGsqO6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"ARIZE_SPACE_ID\"] = userdata.get(\"ARIZE_SPACE_ID\") or getpass(\"üîë Enter your Arize Space ID: \")\n",
        "\n",
        "os.environ[\"ARIZE_API_KEY\"] = userdata.get(\"ARIZE_API_KEY\") or getpass(\"üîë Enter your Arize API Key: \")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\") or getpass(\"üîë Enter your OpenAI API Key: \")\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\") or getpass(\"üîë Enter your Tavily API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVQ4WfbyaIAB"
      },
      "source": [
        "Note that we are tracing our agent outputs to a different project from previous labs here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PZNGrszQJ-n"
      },
      "outputs": [],
      "source": [
        "from arize.otel import register, Transport\n",
        "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "from openinference.instrumentation.agno import AgnoInstrumentor\n",
        "\n",
        "model_id = \"evaluate-travel-agent\"\n",
        "tracer_provider = register(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    api_key=os.getenv(\"ARIZE_API_KEY\"),\n",
        "    project_name=model_id,\n",
        "    set_global_tracer_provider=True,\n",
        "    log_to_console=True,\n",
        "    endpoint=\"https://otlp.ca-central-1a.arize.com/v1/traces\",\n",
        "    transport=Transport.HTTP\n",
        ")\n",
        "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "AgnoInstrumentor().instrument(tracer_provider=tracer_provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogbl-o3IQ90I"
      },
      "source": [
        "# Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrAqXWtxQLyl"
      },
      "outputs": [],
      "source": [
        "# --- Helper functions for tools ---\n",
        "import httpx\n",
        "from opentelemetry import trace\n",
        "\n",
        "tracer = trace.get_tracer(__name__)\n",
        "\n",
        "@tracer.chain(name=\"search-api\")\n",
        "def _search_api(query: str) -> str | None:\n",
        "    \"\"\"Try Tavily search first, fall back to None.\"\"\"\n",
        "    tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "    if not tavily_key:\n",
        "        return None\n",
        "    try:\n",
        "        resp = httpx.post(\n",
        "            \"https://api.tavily.com/search\",\n",
        "            json={\n",
        "                \"api_key\": tavily_key,\n",
        "                \"query\": query,\n",
        "                \"max_results\": 3,\n",
        "                \"search_depth\": \"basic\",\n",
        "                \"include_answer\": True,\n",
        "            },\n",
        "            timeout=8,\n",
        "        )\n",
        "        data = resp.json()\n",
        "        answer = data.get(\"answer\") or \"\"\n",
        "        snippets = [r.get(\"content\", \"\") for r in data.get(\"results\", [])]\n",
        "        combined = \" \".join([answer] + snippets).strip()\n",
        "        return combined[:400] if combined else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _compact(text: str, limit: int = 200) -> str:\n",
        "    \"\"\"Compact text for cleaner outputs.\"\"\"\n",
        "    cleaned = \" \".join(text.split())\n",
        "    return cleaned if len(cleaned) <= limit else cleaned[:limit].rsplit(\" \", 1)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAXZhxYFQNBJ"
      },
      "outputs": [],
      "source": [
        "# --- APIs for Essential Info Tool ---\n",
        "import httpx\n",
        "from urllib.parse import quote\n",
        "from typing import Optional\n",
        "\n",
        "@tracer.chain(name=\"wiki-summary-api\")\n",
        "def _wiki_summary(dest: str) -> str:\n",
        "    if not dest:\n",
        "        return \"\"\n",
        "    encoded_dest = quote(dest)\n",
        "\n",
        "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{encoded_dest}\"\n",
        "    HEADERS = { 'User-Agent': 'MyArizeApp/1.0 (ExampleContac@example.com)'}\n",
        "\n",
        "    try:\n",
        "        r = httpx.get(url, headers = HEADERS, timeout=5)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        data = r.json().get(\"extract\")\n",
        "        return data if data else \"\"\n",
        "\n",
        "    except httpx.HTTPStatusError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            return \"\"\n",
        "        return \"\"\n",
        "    except httpx.RequestError as e:\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "@tracer.chain(name=\"weather-api\")\n",
        "def _weather(dest):\n",
        "    g = httpx.get(f\"https://geocoding-api.open-meteo.com/v1/search?name={dest}\")\n",
        "    if g.status_code != 200 or not g.json().get(\"results\"):\n",
        "        return \"\"\n",
        "    lat, lon = g.json()[\"results\"][0][\"latitude\"], g.json()[\"results\"][0][\"longitude\"]\n",
        "    w = httpx.get(f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\").json()\n",
        "    cw = w.get(\"current_weather\", {})\n",
        "    return f\"Weather now: {cw.get('temperature')}¬∞C, wind {cw.get('windspeed')} km/h.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b99WNnmEXfgU"
      },
      "outputs": [],
      "source": [
        "from agno.tools import tool\n",
        "\n",
        "@tool\n",
        "def essential_info(destination: str) -> str:\n",
        "    \"\"\"Get essential info (summary and weather) using APIs\"\"\"\n",
        "    parts = []\n",
        "    wiki = _wiki_summary(destination)\n",
        "    if wiki: parts.append(wiki)\n",
        "    weather = _weather(destination)\n",
        "    if weather: parts.append(weather)\n",
        "    return f\"{destination} essentials:\\n\" + \"\\n\".join(parts)\n",
        "\n",
        "@tool\n",
        "def budget_basics(destination: str, duration: str) -> str:\n",
        "    \"\"\"Summarize travel cost categories.\"\"\"\n",
        "    q = f\"{destination} travel budget average daily costs {duration}\"\n",
        "    s = _search_api(q)\n",
        "    if s:\n",
        "        return f\"{destination} budget ({duration}): {_compact(s)}\"\n",
        "    return f\"Budget for {duration} in {destination} depends on lodging, meals, transport, and attractions.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNkrWM89RAA1"
      },
      "source": [
        "# Create RAG System for Local Flavor Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn0R4FHFUpG7"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create collection for local guides\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"local_guides\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RAG system initialized with ChromaDB and sentence-transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKJOkyQ1REek"
      },
      "source": [
        "Upload `local_flavor.json` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abGYubKMTdO5"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "guide = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPykxefuRKbP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_and_index_guides():\n",
        "\n",
        "    with open('local_guides.json', 'r') as f:\n",
        "      guides = json.load(f)\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "\n",
        "    for i, guide in enumerate(guides):\n",
        "        # Create a rich text representation for embedding\n",
        "        text = f\"City: {guide['city']}. Interests: {', '.join(guide['interests'])}. Experience: {guide['description']}\"\n",
        "\n",
        "        documents.append(text)\n",
        "        metadatas.append({\n",
        "          \"city\": guide[\"city\"],\n",
        "          \"interests\": \", \".join(guide[\"interests\"]),\n",
        "          \"source\": guide[\"source\"],\n",
        "          \"description\": guide[\"description\"]\n",
        "        })\n",
        "        ids.append(f\"guide_{i}\")\n",
        "\n",
        "    # Add to ChromaDB collection\n",
        "    collection.add(\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        ids=ids\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Indexed {len(documents)} experiences in vector database\")\n",
        "    return len(documents)\n",
        "\n",
        "# Load the data\n",
        "num_guides = load_and_index_guides()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdNq15_QXbDF"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from openinference.semconv.trace import SpanAttributes, DocumentAttributes\n",
        "\n",
        "# Initialize embedding model (same one you used for indexing)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "@tool\n",
        "def local_flavor(destination: str, interests: str = \"local culture\") -> str:\n",
        "    \"\"\"Suggest authentic local experiences using vector retrieval from Chroma.\"\"\"\n",
        "    with tracer.start_as_current_span(name=\"RAG\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"RETRIEVER\"}) as span:\n",
        "      # Construct the query text\n",
        "      query_text = f\"{destination} {interests} authentic experiences\"\n",
        "      span.set_attribute(SpanAttributes.INPUT_VALUE, query_text)\n",
        "\n",
        "      # Embed the query\n",
        "      query_embedding = embedding_model.encode([query_text])\n",
        "\n",
        "      # Search in Chroma collection\n",
        "      results = collection.query(\n",
        "          query_embeddings=query_embedding,\n",
        "          n_results=3  # how many guides to retrieve\n",
        "      )\n",
        "\n",
        "      if not results or not results.get(\"documents\"):\n",
        "          return f\"Explore {destination}'s unique {interests} through markets, neighborhoods, and local eateries.\"\n",
        "\n",
        "      # Extract retrieved guides\n",
        "      retrieved_docs = results[\"documents\"][0]\n",
        "      retrieved_meta = results[\"metadatas\"][0]\n",
        "      for i, doc in enumerate(retrieved_docs):\n",
        "        span.set_attribute(f\"retrieval.documents.{i}.document.id\", f\"doc_{i}\")\n",
        "        span.set_attribute(f\"retrieval.documents.{i}.document.content\", doc)\n",
        "\n",
        "      # Format a summary\n",
        "      suggestions = []\n",
        "      for doc, meta in zip(retrieved_docs, retrieved_meta):\n",
        "          suggestion = f\"üìç **{meta['city']}** ‚Äî {meta['description']} (Interests: {meta['interests']})\"\n",
        "          suggestions.append(suggestion)\n",
        "\n",
        "      response = f\"Here are some authentic {interests} experiences near {destination}:\\n\\n\" + \"\\n\\n\".join(suggestions)\n",
        "      span.set_attribute(SpanAttributes.OUTPUT_VALUE, response)\n",
        "\n",
        "      return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5NL30n6RJN3"
      },
      "source": [
        "# Define Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOPVdh4dQOjB"
      },
      "outputs": [],
      "source": [
        "from agno.agent import Agent\n",
        "from agno.models.openai import OpenAIChat\n",
        "\n",
        "# --- Main Agent ---\n",
        "trip_agent = Agent(\n",
        "    name=\"TripPlanner\",\n",
        "    role=\"AI Travel Assistant\",\n",
        "    model=OpenAIChat(id=\"gpt-4.1\"),\n",
        "    instructions=(\n",
        "        \"You are a friendly and knowledgeable travel planner. \"\n",
        "        \"Combine multiple tools to create a trip plan including essentials, budget, and local flavor. \"\n",
        "        \"Keep the tone natural, clear, and under 1000 words.\"\n",
        "    ),\n",
        "    markdown=True,\n",
        "    tools=[essential_info, budget_basics, local_flavor],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb25jqntwMPo"
      },
      "source": [
        "# Evaluate the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CwQvpyYCFd5"
      },
      "source": [
        "The first step before evaluating our agent is to generate multiple runs using different query types. This way, our evaluation will cover many different cases. Running the two cells below will send requests to the agent and create traces that will appear in Arize. Unlike previous labs, these traces will be logged under a new project titled ‚Äú**evaluate-travel-agent**‚Äù."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpgEqoe28V_j"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"Plan a 5-day trip to Dubai. Focus on history, wellness. Include essential info, budget breakdown, and local experiences.\",\n",
        "    \"Plan a 6-day trip to Dubai. Focus on art, heritage sites, and sustainability. Include recommendations for cultural districts and green hotels.\",\n",
        "    \"Plan a 4-day trip to Bangkok. Focus on history, floating markets, and photography spots. Include essential travel info.\",\n",
        "    \"Plan a 6-day trip to Bangkok. Focus on art, hidden caf√©s, and authentic experiences. Include budget options and local insights.\",\n",
        "    \"Plan a 5-day trip to Prague. Focus on history, beer culture, and architecture. Include daily breakdown and cultural tips.\",\n",
        "    \"Plan a 3-day trip to Prague. Focus on castles, local cuisine, and romantic spots. Include estimated costs and best walking routes.\",\n",
        "    \"Plan a 3-day trip to Barcelona. Focus on food tours and Gaud√≠ landmarks. Include costs and top attractions.\",\n",
        "    \"Plan a 4-day trip to Barcelona. Focus on wellness, yoga, and beach relaxation. Include daily schedule and spa recommendations.\",\n",
        "    \"Plan a 5-day trip to Tokyo. Focus on history, modern tech, and wellness. Include itinerary and budget details.\",\n",
        "    \"Plan a 6-day trip to Tokyo. Focus on innovation, culture, and hidden gems. Include budget summary and cultural etiquette.\",\n",
        "    \"Plan a 3-day trip to Rome. Focus on ancient ruins, espresso culture, and walking tours. Include itinerary and budget guide.\",\n",
        "    \"Plan a 6-day trip to Rome. Focus on spirituality, history, and Italian cuisine. Include detailed breakdown and safety advice.\",\n",
        "    \"Plan a 3-day trip to Lisbon. Focus on tram rides, fado music, and street art. Include daily plan and estimated budget.\",\n",
        "    \"Plan a 6-day trip to Lisbon. Focus on cuisine, culture, and nightlife. Include recommendations for authentic spots.\",\n",
        "    \"Plan a 5-day trip to New York. Focus on museums, food, and nightlife. Include itinerary and average daily costs.\",\n",
        "    \"Plan a 6-day trip to New York. Focus on photography, cuisine, and local markets. Include safety tips and budget advice.\",\n",
        "    \"Plan a 5-day trip to Marrakech. Focus on history, wellness, and local crafts. Include essential info, budget, and itinerary.\",\n",
        "    \"Plan a 3-day trip to Marrakech. Focus on souks, cuisine, and architecture. Include costs and cultural insights.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNvq3NS4tEi5"
      },
      "outputs": [],
      "source": [
        "for q in queries:\n",
        "  response = trip_agent.run(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhAuDY8ftQ7s"
      },
      "source": [
        "# Span-Level Evaluation via Arize Python SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OU9W_50cx8"
      },
      "source": [
        "Arize supports evaluations at multiple levels of granularity. You can evaluate individual steps in an agent‚Äôs run (spans) or the full workflow (trace).\n",
        "\n",
        "Here, we‚Äôll perform span-level evaluations on retrieval steps to measure how relevant the retrieved documents are to each query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r902HNVxVwu"
      },
      "source": [
        "First, navigate to your project and click \"Export to Notebook\". From here, copy the `export_model_to_df` function in the code snippet to export your traces.\n",
        "![Export Traces](https://storage.googleapis.com/arize-phoenix-assets/assets/images/export-traces-arize.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlwUzBz_zrQ2"
      },
      "outputs": [],
      "source": [
        "from arize.exporter import ArizeExportClient\n",
        "from datetime import datetime\n",
        "from arize.utils.types import Environments\n",
        "\n",
        "client = ArizeExportClient()\n",
        "print('#### Exporting your primary dataset into a dataframe.')\n",
        "\n",
        "# INSERT COPY AND PASTED FUNCTION HERE\n",
        "primary_df = client.export_model_to_df(\n",
        "    space_id='U3BhY2U6MTUzMDU6dDJJWg==',\n",
        "    model_id='evaluate-travel-agent',\n",
        "    environment=Environments.TRACING,\n",
        "    start_time=datetime.fromisoformat('2025-11-18T08:00:00.000+00:00'),\n",
        "    end_time=datetime.fromisoformat('2025-11-26T07:59:59.999+00:00'),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAau9WZGXdC6"
      },
      "source": [
        "Next, we define the prompt template for our LLM Judge. Feel free to customize this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6h1VwxN1ipb"
      },
      "outputs": [],
      "source": [
        "RAG_RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
        "You are comparing a reference text to a question and trying to determine if the reference text\n",
        "contains information relevant to answering the question. Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question]: {{input}}\n",
        "    ************\n",
        "    [Reference text]: {{documents}}\n",
        "    ************\n",
        "    [END DATA]\n",
        "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
        "contains information that can help answer the Question. First, write out in a step by step manner\n",
        "an EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\n",
        "at the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\n",
        "should not contain any text or characters aside from that word. \"unrelated\" means that the\n",
        "reference text does not help answer to the Question. \"relevant\" means the reference text directly\n",
        "answers the question.\n",
        "\n",
        "Example response:\n",
        "LABEL: \"relevant\" or \"unrelated\"\n",
        "************\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPUIbJK1Xi_V"
      },
      "source": [
        "Then, we grab relevant columns from our spans dataframe and rename columns to match the variables in the LLM Judge prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V84b_i620Uew"
      },
      "outputs": [],
      "source": [
        "spans_df = primary_df[\n",
        "    [\n",
        "        \"name\",\n",
        "        \"context.span_id\",\n",
        "        \"attributes.openinference.span.kind\",\n",
        "        \"context.trace_id\",\n",
        "        \"attributes.input.value\",\n",
        "        \"attributes.retrieval.documents\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cxpnQE7F1MgE"
      },
      "outputs": [],
      "source": [
        "filtered_df = spans_df[\n",
        "    (spans_df[\"attributes.openinference.span.kind\"] == \"RETRIEVER\")\n",
        "    & (spans_df[\"attributes.retrieval.documents\"].notnull())\n",
        "]\n",
        "\n",
        "filtered_df = filtered_df.rename(\n",
        "    columns={\"attributes.input.value\": \"input\", \"attributes.retrieval.documents\": \"documents\"}\n",
        ")\n",
        "\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gH_laVQXtYS"
      },
      "source": [
        "Finally, we define our evaluators and run the evaluation. When the evaluation is done running, we log the results back to Arize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53MGDwh51kyd"
      },
      "outputs": [],
      "source": [
        "from openinference.instrumentation import suppress_tracing\n",
        "from phoenix.evals.evaluators import async_evaluate_dataframe\n",
        "from phoenix.evals.llm import LLM\n",
        "from phoenix.evals import create_classifier\n",
        "\n",
        "llm = LLM(provider=\"openai\", model=\"gpt-5\")\n",
        "\n",
        "relevancy_evaluator = create_classifier(\n",
        "    name=\"RAG Relevancy\",\n",
        "    llm=llm,\n",
        "    prompt_template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "    choices={\"relevant\": 1.0, \"unrelated\": 0.0},\n",
        ")\n",
        "\n",
        "with suppress_tracing():\n",
        "    results_df = await async_evaluate_dataframe(\n",
        "        dataframe=filtered_df,\n",
        "        evaluators=[relevancy_evaluator],\n",
        "    )\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJXxkO363uaS"
      },
      "outputs": [],
      "source": [
        "from arize.pandas.logger import Client\n",
        "from phoenix.evals.utils import to_annotation_dataframe\n",
        "import ast\n",
        "\n",
        "import pandas as pd\n",
        "client = Client()\n",
        "\n",
        "rag_eval_df = to_annotation_dataframe(results_df)\n",
        "rag_eval_df = rag_eval_df.rename(columns={\n",
        "    \"label\": \"eval.rag.label\",\n",
        "    \"score\": \"eval.rag.score\",\n",
        "    \"explanation\": \"eval.rag.explanation\",\n",
        "    \"metadata\": \"eval.rag.metadata\"\n",
        "})\n",
        "\n",
        "client.log_evaluations_sync(rag_eval_df, 'evaluate-travel-agent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-DwN3jX1sM"
      },
      "source": [
        "Click on the retriever spans within each trace to view detailed evaluation results. You can also filter by evaluation outcome to quickly identify which queries successfully retrieved the most relevant documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmX77R7f5jvm"
      },
      "source": [
        "![Eval Result](https://storage.googleapis.com/arize-phoenix-assets/assets/images/arize-course-lab6-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI8KH2i65qWh"
      },
      "source": [
        "# Trace-Level Evaluation in the Arize UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6laqRpu_gmD"
      },
      "source": [
        "### In this section, we will walk you through how to set up and run evaluations in the Arize UI. Specifically, we will be running a trace level evaluation to determine the answer quality of our agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnBqv0cu-1iG"
      },
      "source": [
        "<video width=\"940\" height=\"680\" controls>\n",
        "  <source src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/videos/trace-level-evals-course.mp4\" type=\"video/mp4\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0EAnU9BAT_g"
      },
      "source": [
        "1. In the project containing your traces, go to Eval Tasks and select LLM as a Judge.\n",
        "\n",
        "2. Name your task and schedule it to run on historical data. Each task can include multiple evaluators, but this walkthrough focuses on setting up one.\n",
        "\n",
        "3. Choose a trace-level evaluation.\n",
        "\n",
        "4. From the predefined templates, select Q&A or another template of your choice. You can also create a custom evaluation. If you define your own, ensure the variables align with your trace structure and specify the output labels (rails).\n",
        "\n",
        "5. Click Create Evals. Your evaluations will begin running and will appear on your existing traces. Look for the eval result on the top span for each trace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYC5vXAZuCp"
      },
      "source": [
        "![Trace Level Eval](https://storage.googleapis.com/arize-phoenix-assets/assets/images/trace-level-evals-ui-course.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}